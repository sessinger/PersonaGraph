{
 "metadata": {
  "name": "",
  "signature": "sha256:8a5e5058e268985014086a74eb77b87d40992482b9d551e5f7e7859eab18ad4c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Fitness Text Classification"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Approach"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Convert training and test data to lower case (Normalize)\n",
      "# Perform stemming on training and test data\n",
      "# Perform lemmatization on training and test data\n",
      "# Perform stop word removal on training and test data\n",
      "# Experiment with bag-of-words counts vs tf-idf transform\n",
      "# ngram tuning\n",
      "# Have equal amount of samples in each category of the training data (Statified KFold)\n",
      "# Alternatively, evaluate performance on 70/30 split \n",
      "# Compare Ridge Classifier, Naive Bayes, Logistic Regression and Ensemble with Equal Weighting"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Best Performance: 0.932 FScore - 10-Fold CV Averaged"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Normalized Data\n",
      "# Lemmatization\n",
      "# Stop word Removal\n",
      "# ngram set 3 (word)\n",
      "# tf-idf transform\n",
      "# Ridge Classifier"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import print_function\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn import cross_validation\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.linear_model import RidgeClassifier, LogisticRegression\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn import metrics\n",
      "\n",
      "from nltk import word_tokenize\n",
      "from nltk.stem.porter import PorterStemmer\n",
      "from nltk.stem.wordnet import WordNetLemmatizer\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "from scipy.spatial.distance import hamming\n",
      "import scipy.stats as spy"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Data Parsing and Preprocessing"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stop = stopwords.words()\n",
      "stemmer = PorterStemmer()\n",
      "lemma = WordNetLemmatizer()\n",
      "\n",
      "data_file = open('data.txt','r')\n",
      "\n",
      "data = []\n",
      "for line in data_file:\n",
      "    line = line.strip()\n",
      "    line = line.lower()\n",
      "    line = line.split()\n",
      "    #tokens = [stemmer.stem(t) for t in line]\n",
      "    tokens = [lemma.lemmatize(t) for t in line] # Better generalization performance than stemming or combined\n",
      "    stringed = ' '.join(tokens)\n",
      "    data.append(stringed)\n",
      "data_file.close()\n",
      "data = np.asarray(data)\n",
      "\n",
      "labels_file = open('labels.txt','r')\n",
      "labels = []\n",
      "for line in labels_file:\n",
      "    labels.append(line.strip())\n",
      "labels_file.close()\n",
      "labels = np.asarray(labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Classifier Helper Function\n",
      "def RunClassifier(clf):\n",
      "    clf.fit(X_train, y_train)\n",
      "    pred = clf.predict(X_test)\n",
      "    return metrics.f1_score(y_test, pred), pred"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Stratified 10-Fold Cross Validation (FScore)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "Ensures each fold has equal amount of positive and negative examples"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "kf = cross_validation.StratifiedKFold(labels, n_folds = 10)\n",
      "vectorizer = TfidfVectorizer(ngram_range = (1,3), stop_words='english', sublinear_tf=True) # Remove Stop words and set ngram range\n",
      "#vectorizer = CountVectorizer(ngram_range = (1,3), stop_words='english') # Remove Stop words and set ngram range\n",
      "\n",
      "RC, NB, LR, EN = [], [], [], []\n",
      "for train_index, test_index in kf:\n",
      "    \n",
      "    X_train, X_test = data[train_index], data[test_index]\n",
      "    y_train, y_test = labels[train_index].astype('float'), labels[test_index].astype('float')\n",
      "    X_train = vectorizer.fit_transform(X_train)\n",
      "    X_test = vectorizer.transform(X_test)\n",
      "    \n",
      "    resultRC = RunClassifier(RidgeClassifier(tol=1e-2, solver=\"lsqr\"))\n",
      "    RC.append(resultRC[0])\n",
      "    RCP = resultRC[1]\n",
      "    \n",
      "    resultNB = RunClassifier(MultinomialNB(alpha=0.01))\n",
      "    NB.append(resultNB[0])\n",
      "    NBP = resultNB[1]\n",
      "    \n",
      "    resultLR = RunClassifier(LogisticRegression(penalty='l2'))\n",
      "    LR.append(resultLR[0])\n",
      "    LRP = resultLR[1]\n",
      "\n",
      "    combined = spy.mode(np.vstack((RCP,NBP,LRP)))\n",
      "    EN.append(metrics.f1_score(y_test, combined[0][0]))\n",
      "    ENP = combined[0][0]\n",
      "    \n",
      "print('RidgeClassifier: %s' %round(np.asarray(RC).mean(),3))\n",
      "print('MultinomialNB: %s' %round(np.asarray(NB).mean(),3))\n",
      "print('LogisticRegression: %s' %round(np.asarray(LR).mean(),3))\n",
      "print('Ensemble: %s' %round(np.asarray(EN).mean(),3))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "RidgeClassifier: 0.932\n",
        "MultinomialNB: 0.917\n",
        "LogisticRegression: 0.926\n",
        "Ensemble: 0.928\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Classification 70/30 Split"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train_temp, X_test_temp, y_train, y_test = cross_validation.train_test_split(data, labels, test_size=0.3, random_state=10)\n",
      "y_train = y_train.astype('float')\n",
      "y_test = y_test.astype('float')\n",
      "\n",
      "vectorizer = TfidfVectorizer(ngram_range = (1,3), stop_words='english')\n",
      "\n",
      "X_train = vectorizer.fit_transform(X_train_temp)\n",
      "X_test = vectorizer.transform(X_test_temp)\n",
      "\n",
      "resultRC = RunClassifier(RidgeClassifier(tol=1e-2, solver=\"lsqr\"))\n",
      "RCP = resultRC[1]\n",
      "\n",
      "resultNB = RunClassifier(MultinomialNB(alpha=0.01))\n",
      "NBP = resultNB[1]\n",
      "\n",
      "resultLR = RunClassifier(LogisticRegression(penalty='l2'))\n",
      "LRP = resultLR[1]\n",
      "\n",
      "combined = spy.mode(np.vstack((RCP,NBP,LRP)))\n",
      "EN.append(metrics.f1_score(y_test, combined[0][0]))\n",
      "ENP = combined[0][0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Performance Metrics on 30% Test Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def evaluate(truth, pred):\n",
      "    R = confusion_matrix(truth,pred)\n",
      "    TP = R[0,0].astype('float')\n",
      "    FP = R[0,1].astype('float')\n",
      "    TN = R[1,1].astype('float')\n",
      "    FN = R[1,0].astype('float')\n",
      "\n",
      "    Sensitivity = TP/(TP + FN)\n",
      "    Specificity = TN/(TN + FP)\n",
      "\n",
      "    Precision = TP/(TP + FP)\n",
      "    Recall = Sensitivity\n",
      "    FScore = 2*(Precision*Recall)/(Precision+Recall)\n",
      "\n",
      "    Accuracy = (TP+ TN)/(TP+ TN + FP + FN)\n",
      "    Hamming_Loss = hamming(y_test,ENP) # Also 1-Accuracy\n",
      "\n",
      "    print('Accuracy: %s' %round(Accuracy,3))\n",
      "    print('FScore: %s' %round(FScore,3))\n",
      "    print('Precision: %s' %round(Precision,3))\n",
      "    print('Recall: %s' %round(Recall,3))\n",
      "    print('Sensitivity: %s' %round(Sensitivity,3))\n",
      "    print('Specificity: %s' %round(Specificity,3))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Ridge Classifier"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "evaluate(y_test,RCP)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Accuracy: 0.922\n",
        "FScore: 0.921\n",
        "Precision: 0.925\n",
        "Recall: 0.917\n",
        "Sensitivity: 0.917\n",
        "Specificity: 0.927\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Naive Bayes"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "evaluate(y_test,NBP)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Accuracy: 0.914\n",
        "FScore: 0.913\n",
        "Precision: 0.918\n",
        "Recall: 0.907\n",
        "Sensitivity: 0.907\n",
        "Specificity: 0.92\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Logistic Regression"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "evaluate(y_test,LRP)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Accuracy: 0.914\n",
        "FScore: 0.914\n",
        "Precision: 0.938\n",
        "Recall: 0.892\n",
        "Sensitivity: 0.892\n",
        "Specificity: 0.936\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Ensemble: Majority Vote"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "evaluate(y_test,ENP)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Accuracy: 0.916\n",
        "FScore: 0.916\n",
        "Precision: 0.925\n",
        "Recall: 0.906\n",
        "Sensitivity: 0.906\n",
        "Specificity: 0.926\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "McNemar Test for Significant Difference Between Classifier Classification Accuracy"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy import stats\n",
      "    \n",
      "def mcnemar(a,b,c,d):\n",
      "    \"\"\"\n",
      "    Input args:\n",
      "       a, b, c, d - counts\n",
      "        a: test 1 failed, test 2 passed\n",
      "        b: test 1 failed, test 2 failed\n",
      "        c: test 1 passed, test 2 passed\n",
      "        d: test 1 passed, test 2 failed\n",
      "    Output:\n",
      "       pvalue of test.\n",
      "    \"\"\"\n",
      "    if b+c < 25:\n",
      "        return 'Use binomial exact test.'\n",
      "    chi2testval = (abs(a-d) - 1)**2/ (a + d)\n",
      "    df = 1\n",
      "    pvalue = 1 - stats.chi2.cdf(chi2testval, df)\n",
      "    return pvalue\n",
      "\n",
      "def compare_classifier(truth,M1,M2):\n",
      "    \"\"\"\n",
      "    Input args:\n",
      "        M1, M2 - binary predictions for methods 1 and 2\n",
      "        truth - binary ground truth\n",
      "    Output:\n",
      "       pvalue of mcnemar test (if <= 0.05 then significant difference)\n",
      "    \"\"\"\n",
      "    # a: test 1 failed, test 2 passed\n",
      "    a1 = np.nonzero((truth==1) & (M1==0) & (M2==1))[0].shape[0]\n",
      "    a2 = np.nonzero((truth==0) & (M1==1) & (M2==0))[0].shape[0]\n",
      "    a = float(a1+a2)\n",
      "\n",
      "    # b: test 1 failed, test 2 failed\n",
      "    b1 = np.nonzero((truth==1) & (M1==0) & (M2==0))[0].shape[0]\n",
      "    b2 = np.nonzero((truth==0) & (M1==1) & (M2==1))[0].shape[0]\n",
      "    b = float(b1+b2)\n",
      "\n",
      "    # c: test 1 passed, test 2 passed\n",
      "    c1 = np.nonzero((truth==1) & (M1==1) & (M2==1))[0].shape[0]\n",
      "    c2 = np.nonzero((truth==0) & (M1==0) & (M2==0))[0].shape[0]\n",
      "    c = float(c1+c2)\n",
      "\n",
      "    # d: test 1 passed, test 2 failed\n",
      "    d1 = np.nonzero((truth==1) & (M1==1) & (M2==0))[0].shape[0]\n",
      "    d2 = np.nonzero((truth==0) & (M1==0) & (M2==1))[0].shape[0]\n",
      "    d = float(d1+d2)\n",
      "\n",
      "    return mcnemar(a,b,c,d)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Ridge vs. Ensemble"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# P-value > 0.05 so no statistical significant difference\n",
      "print(compare_classifier(y_test,RCP,ENP))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.182422439452\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Ridge vs. Logistic"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# P-value > 0.05 so no statistical significant difference\n",
      "print(compare_classifier(y_test,LRP,RCP))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.190430263826\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Ridge vs. Naive Bayes"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# P-value > 0.05 so no statistical significant difference\n",
      "print(compare_classifier(y_test,NBP,RCP))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.323939845599\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Next Step: try Latent Symantic Analysis to improve performance"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}